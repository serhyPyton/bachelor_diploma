\chapter{Розв'язання задачі методом найменших квадратів}

Другий розділ присвячено розв'язку задачі ідентифікації випадкових відображень
точкових множин методом найменших квадратів.
Досліджуються властивості оцінок найменших квадратів та з'ясовується придатність
розв'язку для поставленої задачі.

\section{Оцінка невідомих параметрів задачі методом найменших квадратів}

Оскільки множини $S$ і $T$ скінченні,
скористаємося звичайним методом найменших квадратів
\cite{hudson:least-squares}.
Оцінимо невідомі параметри у виразі \eqref{eq:problem}.
Для цього знайдемо мінімум суми квадратів похибок
\begin{equation}\label{eq:least:squares}
 E \left( k, R, \boldsymbol{b} \right) =
 \sum \limits_{\boldsymbol{s} \in S} \left \Vert \boldsymbol{\xi_s} \right \Vert^2 =
 \sum \limits_{\boldsymbol{s} \in S}
  \left \Vert \boldsymbol{k_s} - R \cdot \boldsymbol{s} - \boldsymbol{b} \right \Vert^2 \to
 \min \limits_{k, R, \boldsymbol{b}}.
\end{equation}

Сума квадратів евклідових відстаней між векторами~---~це те ж саме,
що і сума квадратів відхилень між проекціями на кожну вісь
\begin{equation*}
  E \left( k, R, \boldsymbol{b} \right) =
    E_x \left( k, R, \boldsymbol{b} \right) + E_y \left( k, R, \boldsymbol{b} \right) +
    E_z \left( k, R, \boldsymbol{b} \right) \to
    \min \limits_{k, R, \boldsymbol{b}}.
\end{equation*}

Знайдемо, чому дорівнює добуток матриці $R$ та вектора $\boldsymbol{s}$
\begin{equation*}
  R \cdot \boldsymbol{s} =
  \begin{bmatrix}
    r_{xx} & r_{xy} & r_{xz} \\
    r_{yx} & r_{yy} & r_{yz} \\
    r_{zx} & r_{zy} & r_{zz}
  \end{bmatrix} \cdot
  \begin{bmatrix}
    s_x \\
    s_y \\
    s_z
  \end{bmatrix} =
  \begin{bmatrix}
    r_{xx} \cdot s_x + r_{xy} \cdot s_y + r_{xz} \cdot s_z \\
    r_{yx} \cdot s_x + r_{yy} \cdot s_y + r_{yz} \cdot s_z \\
    r_{zx} \cdot s_x + r_{zy} \cdot s_y + r_{zz} \cdot s_z
  \end{bmatrix} =
  \begin{bmatrix}
    \boldsymbol{r}_x \cdot \boldsymbol{s} \\
    \boldsymbol{r}_y \cdot \boldsymbol{s} \\
    \boldsymbol{r}_z \cdot \boldsymbol{s}
  \end{bmatrix}.
\end{equation*}

Розкладемо суму квадратів відхилень \eqref{eq:least:squares} на проекції
\begin{gather*}
  E \left( k, R, \boldsymbol{b} \right) =
  \sum \limits_{\boldsymbol{s} \in S} \left(
    \boldsymbol{r}_x \cdot \boldsymbol{s} + \boldsymbol{r}_y \cdot \boldsymbol{s} +
    \boldsymbol{r}_z \cdot \boldsymbol{s} + b_x + b_y + b_z - k_{s_x} - k_{s_y} - k_{s_z}
  \right)^2 = \\
  = \sum \limits_{\boldsymbol{s} \in S} \left[
    \left( \boldsymbol{r}_x \cdot \boldsymbol{s} + b_x - k_{s_x} \right) +
    \left( \boldsymbol{r}_y \cdot \boldsymbol{s} + b_y - k_{s_y} \right) +
    \left( \boldsymbol{r}_z \cdot \boldsymbol{s} + b_z - k_{s_z} \right) \right]^2 = \\
  = \sum \limits_{\boldsymbol{s} \in S}
      \left( \boldsymbol{r}_x \cdot \boldsymbol{s} + b_x - k_{s_x} \right)^2 +
    \sum \limits_{\boldsymbol{s} \in S}
      \left( \boldsymbol{r}_y \cdot \boldsymbol{s} + b_y - k_{s_y} \right)^2 +
    \sum \limits_{\boldsymbol{s} \in S}
      \left( \boldsymbol{r}_z \cdot \boldsymbol{s} + b_z - k_{s_z} \right)^2.
\end{gather*}
Множини параметрів, які входять в кожну з трьох сум, різні,
тому можемо мінімізувати суми квадратів проекцій відхилень на
кожну координату окремо
\begin{align*}
  \begin{cases}
    E_x =
    \sum \limits_{\boldsymbol{s} \in S}
      \left( \boldsymbol{r}_x \cdot \boldsymbol{s} + b_x - k_{s_x} \right)^2 &\to
    \min \limits_{\boldsymbol{r}_x, b_x}, \\
    E_y =
    \sum \limits_{\boldsymbol{s} \in S}
      \left( \boldsymbol{r}_y \cdot \boldsymbol{s} + b_y - k_{s_y} \right)^2 &\to
    \min \limits_{\boldsymbol{r}_y, b_y}, \\
    E_z =
    \sum \limits_{\boldsymbol{s} \in S}
    \left( \boldsymbol{r}_z \cdot \boldsymbol{s} + b_z - k_{s_z} \right)^2 &\to
    \min \limits_{\boldsymbol{r}_z, b_z}.
  \end{cases}
\end{align*}

Маємо лінійні функції точок вихідної множини, які підносяться до квадрату.
Оскільки це опуклі функції,
візьмемо часткові похідні по $\boldsymbol{r}_i$ та $b_i$ для всіх
$i \in \left\{ x, y, z \right\} $ та прирівняємо їх до нуля, щоб знайти мінімум.
Отримаємо по чотири рівняння для кожної координати.
Без втрати загальності розглянемо ці рівняння для $E_x$.
Екстремальне значення $E_x$
отримаємо в результаті розв'язку системи лінійних алгебраїчних рівнянь
\begin{gather*}
  \begin{cases}
    \frac{ \partial E_x}{ \partial b_x} =
    \sum \limits_{s \in S}
      2 \left( \boldsymbol{r}_x \cdot \boldsymbol{s} + b_x - k_{s_x} \right ) = 0, \\
    \frac{ \partial E_x}{ \partial r_{xx}} =
    \sum \limits_{s \in S}
      2 \left( \boldsymbol{r}_x \cdot \boldsymbol{s} + b_x - k_{s_x} \right ) \cdot s_x = 0, \\
    \frac{ \partial E_x}{ \partial r_{xy}} =
    \sum \limits_{s \in S}
      2 \left( \boldsymbol{r}_x \cdot \boldsymbol{s} + b_x - k_{s_x} \right ) \cdot s_y = 0, \\
    \frac{ \partial E_x}{ \partial r_{xz}} =
    \sum \limits_{s \in S}
      2 \left( \boldsymbol{r}_x \cdot \boldsymbol{s} + b_x - k_{s_x} \right ) \cdot s_z = 0.
  \end{cases}
\end{gather*}

Розв'язуємо перше рівняння відносно $b_x$
\begin{equation*}
  \sum \limits_{\boldsymbol{s} \in S} b_x =
  \sum \limits_{\boldsymbol{s} \in S} \left(k_{s_x} - \boldsymbol{r}_x \cdot \boldsymbol{s} \right ).
\end{equation*}
Спростимо
\begin{equation*}
  \left| S \right| \cdot b_x + \boldsymbol{r}_x \sum \limits_{\boldsymbol{s} \in S} \boldsymbol{s} =
  \sum \limits_{\boldsymbol{s} \in S} k_{s_x}.
\end{equation*}
Розкриємо скалярний добуток
\begin{equation*}
  \left| S \right| \cdot b_x + \sum \limits_{\boldsymbol{s} \in S} r_{xx} \cdot s_x +
  \sum \limits_{\boldsymbol{s} \in S} r_{xy} \cdot s_y +
  \sum \limits_{\boldsymbol{s} \in S} r_{xz} \cdot s_z  =
  \sum \limits_{\boldsymbol{s} \in S} k_{s_x}.
\end{equation*}

Розв'язуємо інші рівняння відносно $r_{xi}$ для
$i \in \left\{ x, y, z \right\} $.
Без втрати загальності знаходимо розв'язок тільки для $r_{xx}$
\begin{equation*}
  \boldsymbol{r}_x \sum \limits_{\boldsymbol{s} \in S} \boldsymbol{s} \cdot s_x =
  \sum \limits_{\boldsymbol{s} \in S} \left( k_{s_x} - b_x \right) \cdot s_x.
\end{equation*}
Розпишемо скалярний добуток
\begin{equation*}
  \sum \limits_{\boldsymbol{s} \in S} r_{xx} \cdot s_x^2 +
  \sum \limits_{\boldsymbol{s} \in S} r_{xy} \cdot s_x \cdot s_y +
  \sum \limits_{\boldsymbol{s} \in S} r_{xz} \cdot s_x \cdot s_z +
  \sum \limits_{\boldsymbol{s} \in S} b_x \cdot s_x =
  \sum \limits_{\boldsymbol{s} \in S} k_{s_x} \cdot s_x.
\end{equation*}
Маємо систему рівнянь
\begin{equation*}
  \begin{cases}
    \left| S \right| \cdot b_x + \sum \limits_{\boldsymbol{s} \in S} r_{xx} \cdot s_x +
    \sum \limits_{\boldsymbol{s} \in S} r_{xy} \cdot s_y +
    \sum \limits_{\boldsymbol{s} \in S} r_{xz} \cdot s_z  =
    \sum \limits_{\boldsymbol{s} \in S} k_{s_x}, \\
    \sum \limits_{\boldsymbol{s} \in S} r_{xx} \cdot s_x^2 +
    \sum \limits_{\boldsymbol{s} \in S} r_{xy} \cdot s_x \cdot s_y +
    \sum \limits_{\boldsymbol{s} \in S} r_{xz} \cdot s_x \cdot s_z +
    \sum \limits_{\boldsymbol{s} \in S} b_x \cdot s_x =
    \sum \limits_{\boldsymbol{s} \in S} k_{s_x} \cdot s_x, \\
    \sum \limits_{\boldsymbol{s} \in S} r_{xx} \cdot s_x \cdot s_y +
    \sum \limits_{\boldsymbol{s} \in S} r_{xy} \cdot s_y^2 +
    \sum \limits_{\boldsymbol{s} \in S} r_{xz} \cdot s_y \cdot s_z +
    \sum \limits_{\boldsymbol{s} \in S} b_x \cdot s_y =
    \sum \limits_{\boldsymbol{s} \in S} k_{s_x} \cdot s_y, \\
    \sum \limits_{\boldsymbol{s} \in S} r_{xx} \cdot s_x \cdot s_z +
    \sum \limits_{\boldsymbol{s} \in S} r_{xy} \cdot s_y \cdot s_z +
    \sum \limits_{\boldsymbol{s} \in S} r_{xz} \cdot s_z^2 +
    \sum \limits_{s \in S} b_x \cdot s_z =
    \sum \limits_{\boldsymbol{s} \in S} k_{s_x} \cdot s_z.
  \end{cases}
\end{equation*}
Запишемо її в матричному вигляді
\begin{equation*}
  \begin{bmatrix}
    \left| S \right| & \sum \limits_{\boldsymbol{s} \in S} s_x & \sum \limits_{\boldsymbol{s} \in S} s_y & \sum \limits_{\boldsymbol{s} \in S} s_z \\
    \sum \limits_{\boldsymbol{s} \in S} s_x & \sum \limits_{\boldsymbol{s} \in S} s_x^2 & \sum \limits_{\boldsymbol{s} \in S} s_x \cdot s_y & \sum \limits_{\boldsymbol{s} \in S} s_x \cdot s_z \\
    \sum \limits_{\boldsymbol{s} \in S} s_y & \sum \limits_{\boldsymbol{s} \in S} s_x \cdot s_y & \sum \limits_{\boldsymbol{s} \in S} s_y^2 & \sum \limits_{\boldsymbol{s} \in S} s_y \cdot s_z \\
    \sum \limits_{\boldsymbol{s} \in S} s_z & \sum \limits_{\boldsymbol{s} \in S} s_x \cdot s_z & \sum \limits_{\boldsymbol{s} \in S} s_y \cdot s_z & \sum \limits_{\boldsymbol{s} \in S} s_z^2
  \end{bmatrix}
  \begin{bmatrix}
    b_x \\
    r_{xx} \\
    r_{xy} \\
    r_{xz}
  \end{bmatrix} =
  \begin{bmatrix}
    \sum \limits_{\boldsymbol{s} \in S} k_{s_x} \\
    \sum \limits_{\boldsymbol{s} \in S} k_{s_x} \cdot s_x \\
    \sum \limits_{\boldsymbol{s} \in S} k_{s_x} \cdot s_y \\
    \sum \limits_{\boldsymbol{s} \in S} k_{s_x} \cdot s_z
  \end{bmatrix}.
\end{equation*}
Введемо позначення
\begin{gather*}
  \sum \limits_{\boldsymbol{s} \in S} s_i = S_i, \qquad i \in \left\{x, y, z \right\}, \\
  \sum \limits_{\boldsymbol{s} \in S} s_i s_j = S_{ij}, \qquad i, j \in \left\{ x, y, z \right\}, \\
  \sum \limits_{\boldsymbol{s} \in S} k_{s_x} = K, \\
  \sum \limits_{\boldsymbol{s} \in S} k_{s_x} \cdot s_i = k_i, \qquad i \in \left\{ x, y, z \right\}.
\end{gather*}
Рівняння прийняло наступний вигляд
\begin{equation*}
  \begin{bmatrix}
    \left| S \right| & S_x & S_y & S_z \\
    S_x & S_{xx} & S_{xy} & S_{xz} \\
    S_y & S_{xy} & S_{yy} & S_{yz} \\
    S_z & S_{xz} & S_{yz} & S_{zz}
  \end{bmatrix}
  \begin{bmatrix}
    b_x \\
    r_{xx} \\
    r_{xy} \\
    r_{xz}
  \end{bmatrix} =
  \begin{bmatrix}
    K \\
    k_x \\
    k_y \\
    k_z
  \end{bmatrix}.
\end{equation*}
Використовуємо метод Крамера для розв'язання системи лінійних рівнянь.
Визначник $ \Delta $
\begin{gather*}
  \Delta =
  \left| S \right| \cdot S_{xx} \cdot S_{yy} \cdot S_{zz} -
  \sum \limits_{i \in \left\{ x, y, z \right\} } L_i +
  2 \cdot \left| S \right| \cdot S_{xy} \cdot S_{xz} \cdot S_{yz} - \\
  - \sum \limits_{i, j, k \in \left\{ x, y, z \right\} } L_{ijk} +
  2 \sum \limits_{i, j \in \left\{ x, y, z \right\} } L_{ij},
\end{gather*}
де введені позначення при
$i, j, k \in \left\{ x, y, z \right\}, \, i \neq j \neq k$
\begin{gather*}
  L_i = S_{jk} \cdot \left( \left| S \right| \cdot S_{ii} - S_i^2 \right), \\
  L_{ij} = S_i \cdot S_j \cdot \left( S_{ij} \cdot S_k - S_{ik} \cdot S_{jk} \right), \\
  L_{ijk} = S_i^2 \cdot S_{jj} \cdot S_{kk}.
\end{gather*}
Визначник $ \Delta_{b}$
\begin{gather*}
  \Delta_b =
  K \cdot S_{xx} \cdot S_{yy} \cdot S_{zz} - \sum \limits_{i \in \left\{ x, y, z \right\} } L_i^b +
  2 \cdot S_{xy} \cdot S_{xz} \cdot S_{yz} -
  \sum \limits_{i, j, k \in \left\{ x, y, z \right\} } L_{ijk}^b + \\
  + \sum \limits_{i, j \in \left\{ x, y, z \right\} } L_{ij}^b +
  \sum \limits_{i, j \in \left\{ x, y, z \right\} } \left( L_{ij}^b \right)',
\end{gather*}
де введені позначення
\begin{gather*}
  L_{ij}^b = S_{ij}^2 \cdot S_{k} \cdot k_k, \\
  L_{ijk}^b = S_i \cdot S_{jj} \cdot S_{kk}, \\
  \left( L_{ij}^b \right)' = \left(
  S_i \cdot k_j + S_k \cdot k_i \right) \cdot \left( S_{ij} \cdot S_{kk} -
  S_{jk} \cdot S_{ik} \right)
\end{gather*}
при $i, j, k \in \left\{ x, y, z \right\}, \, i \neq j \neq k$.
Визначник $ \Delta_{xx}$
\begin{gather*}
  \Delta_{xx} =
  -K \cdot S_x \cdot S_{yy} \cdot S_{zz} + K \cdot S_x \cdot S_{yz}^2 +
  K \cdot S_y \cdot S_{xy} \cdot S_{zz} - K \cdot S_y \cdot_{xz} \cdot S_{yz} - \\
  - K \cdot S_z \cdot S_{xy} \cdot S_{yz} + K \cdot S_{z} \cdot S_{xz} \cdot S_{yy} +
  k_x \cdot \left| S \right| \cdot S_{yy} \cdot S_{zz} -
  k_x \cdot \left| S \right| \cdot S_{yz}^2 - \\
  - k_x \cdot S_y^2 \cdot S_{zz} + 2 \cdot k_x \cdot S_y \cdot S_z \cdot S_{yz} -
  k_x \cdot S_z^2 \cdot S_{yy} - k_y \cdot \left| S \right| \cdot S_{yz} \cdot S_{zz} + \\
  + k_y \cdot \left| S \right| \cdot S_{xz} \cdot S_{yz} + k_y \cdot S_x \cdot S_y \cdot S_{zz} -
  k_y \cdot S_x \cdot S_y \cdot S_{yz} - k_y \cdot S_y \cdot S_z \cdot S_{xz} + \\
  + k_y \cdot S_z^2 \cdot S_{xy} + k_z \cdot \left| S \right| \cdot S_{xy} \cdot S_{yz} -
  k_z \cdot \left| S \right| \cdot S_{xz} \cdot S_{yy} - k_z \cdot S_x \cdot S_y \cdot S_{yz} + \\
  + k_z \cdot S_x \cdot S_z \cdot S_{yy} + k_z \cdot S_y^2 \cdot S_{xz} -
  k_z \cdot S_y \cdot S_z \cdot S_{xy}.
\end{gather*}
Визначник $ \Delta_{xy}$
\begin{gather*}
  \Delta_{xy} =
  K \cdot S_x \cdot S_{xy} \cdot S_{zz} - K \cdot S_x \cdot S_{xz} \cdot S_{yz} -
  K \cdot S_y \cdot S_{xx} \cdot S_{zz} + K \cdot S_y \cdot S_{xz}^2 + \\
  + K \cdot S_z \cdot S_{xx} \cdot S_{yz} - K \cdot S_z \cdot S_{xy} \cdot S_{xz} -
  k_x \cdot \left| S \right| \cdot S_{xy} \cdot S_{zz} +
  k_x \cdot \left| S \right| \cdot S_{xz} \cdot S_{yz} + \\
  + k_x \cdot S_x \cdot S_y \cdot S_{zz} - k_x \cdot S_x \cdot S_z \cdot S_{yz} -
  k_x \cdot S_y \cdot S_z \cdot S_{xz} + k_x \cdot S_z^2 \cdot S_{xy} + \\
  + k_y \cdot \left| S \right| \cdot S_{xx} \cdot S_{zz} -
  k_y \cdot \left| S \right| \cdot S_{xz}^2 - k_y \cdot S_x^2 \cdot S_{zz} +
  2 \cdot k_y \cdot S_x \cdot S_z \cdot S_{xz} - \\
  - k_y \cdot S_x \cdot S_z \cdot S_{xz} - k_y \cdot S_z^2 \cdot S_{xx} -
  k_z \cdot \left| S \right| \cdot S_{xx} \cdot S_{yz} +
  k_z \cdot \left| S \right| \cdot S_{xy} \cdot S_{xz} + \\
  + k_z \cdot S_x^2 \cdot S_{yz} - k_z \cdot S_x \cdot S_y \cdot S_{xz} -
  k_z \cdot S_x \cdot S_z \cdot S_{xy} + k_z \cdot S_y \cdot S_z \cdot S_{xx}.
\end{gather*}
Визначник $ \Delta_{xz}$
\begin{gather*}
  \Delta_{xz} =
  -K \cdot S_x \cdot S_{xy} \cdot S_{yz} + K \cdot S_x \cdot S_{xz} \cdot S_{yy} +
  K \cdot S_y \cdot S_{xx} \cdot S_{yz} - \\
  - K \cdot S_y \cdot S_{xy} \cdot S_{xz} - K \cdot S_z \cdot S_{xx} \cdot S_{yy} +
  K \cdot S_z \cdot S_{xy}^2 + S_x \cdot \left| S \right| \cdot S_{xy} \cdot S_{yz} - \\
  - k_x \cdot \left| S \right| \cdot S_{xz} \cdot S_{yy} - k_x \cdot S_x \cdot S_y \cdot S_{yz} +
  k_x \cdot S_x \cdot S_y \cdot S_{yy} + k_x \cdot S_y^2 \cdot S_{xz} - \\
  - k_x \cdot S_y \cdot S_z \cdot S_{xy} - k_y \cdot \left| S \right| \cdot S_{xx} \cdot S_{yz} +
  k_y \cdot \left| S \right| \cdot S_{xy} \cdot S_{xz} + k_y \cdot S_x^2 \cdot S_{yz} - \\
  - k_y \cdot S_x \cdot S_y \cdot S_{xz} - k_y \cdot S_x \cdot S_z \cdot S_{xy} +
  k_y \cdot S_y \cdot S_z \cdot S_{xx} +
  k_z \cdot \left| S \right| \cdot S_{xx} \cdot S_{yy} - \\
  - k_z \cdot \left| S \right| \cdot S_{xy}^2 - k_z \cdot S_x^2 \cdot S_{yy} +
  2 \cdot k_z \cdot S_x \cdot S_y \cdot S_{xy} - k_z \cdot S_y^2 \cdot S_{xx}.
\end{gather*}
Відомо, що розв'язком є наступні вирази \cite{voyevodin}
\begin{equation*}
  b_x = \frac{ \Delta_b}{ \Delta }, \,
  r_{xx} = \frac{ \Delta_{xx}}{ \Delta }, \,
  r_{xy} = \frac{ \Delta_{xy}}{ \Delta }, \,
  r_{xz} = \frac{ \Delta_{xz}}{ \Delta }.
\end{equation*}

Інші проекції знаходимо аналогічним чином, прирівнюючи часткові похідні від
$E_y$ та $E_z$ до нуля.

Нехай $ \left| S \right| = n$.
Якщо ввести конструкційну матрицю \cite{hudson:least-squares}
\begin{equation*}
  A =
  \begin{bmatrix}
    1      & s_{1x} & s_{1y} & s_{1z} \\
    1      & s_{2x} & s_{2y} & s_{2z} \\
    \dotsc & \dotsc & \dotsc & \dotsc \\
    1      & s_{nx} & s_{ny} & s_{nx}
  \end{bmatrix}
\end{equation*}
та позначити невідомий вектор через
\begin{equation*}
   \boldsymbol{\theta}^T =
   \begin{bmatrix}
     b_x, r_{xx}, r_{xy}, r_{xz}
   \end{bmatrix},
\end{equation*}
то розв'язок можна записати у більш компактному вигляді.
Перепишемо функцію, яку мінімізуємо, через введені позначення
\begin{equation}\label{eq:energy_x}
  E_x =
  \left( \boldsymbol{k}_x - A \cdot \boldsymbol{\theta} \right)^T \cdot
  \left( \boldsymbol{k}_x - A \cdot \boldsymbol{\theta} \right) =
  \boldsymbol{k}_x^T \cdot \boldsymbol{k}_x - 2 \boldsymbol{\theta}^T \cdot A^T \cdot \boldsymbol{k}_x +
  \boldsymbol{\theta}^T \cdot A^T \cdot A \cdot \boldsymbol{\theta},
\end{equation}
де $\boldsymbol{k}_x = \left( k_{s_1 x}, k_{s_2 x}, \dotsc, k_{s_n x} \right)^T $.
Якщо продиференціювати останній вираз по кожному $\theta_i$,
то отримаємо систему рівнянь у матричному вигляді
\begin{equation*}
  -2A^T \cdot \boldsymbol{k}_x + 2A^T \cdot A \cdot \boldsymbol{\theta} = 0,
\end{equation*}
або
\begin{equation}\label{eq:equation_theta}
  \left( A^T \cdot A \right) \cdot \boldsymbol{\theta} = A^T \cdot \boldsymbol{k}_x.
\end{equation}
Звідси маємо оцінку
\begin{equation}\label{eq:solution}
  \hat{\boldsymbol{\theta}} =
  \left( A^T \cdot A \right)^{-1} \cdot A^T \cdot \boldsymbol{k}_x.
\end{equation}

\textbf{Теорема.}
Оцінка найменших квадратів $\hat{\boldsymbol{\theta}}$
\eqref{eq:solution} забезпечує мінімум функції $E$ \eqref{eq:least:squares}
за фіксованої розмітки $k$
\begin{equation*}
  \min \limits_{\boldsymbol{\theta}} E_x \left( \boldsymbol{\theta} \right) =
  E_x \left( \hat{\boldsymbol{\theta}} \right).
\end{equation*}
Якщо $A^T \cdot A$ є матрицею з ненульовим визначником,
то оцінка найменших квадратів $\hat{\boldsymbol{\theta}}$ єдина.

Щоб довести цю теорему, припустимо, що $\boldsymbol{\theta}^*$~---~довільне фіксоване значення
$\boldsymbol{\theta}$.
Тоді з \eqref{eq:energy_x} маємо
\begin{gather*}
  E_x \left( \hat{\boldsymbol{\theta}} \right) =
  \left[
    \boldsymbol{k}_x - A \cdot \boldsymbol{\theta}^* +
    A \left( \boldsymbol{\theta}^* - \boldsymbol{\theta} \right)
  \right]^T \cdot
  \left[
    \boldsymbol{k}_x - A \cdot \boldsymbol{\theta}^* +
    A \left( \boldsymbol{\theta}^* - \boldsymbol{\theta} \right)
  \right] = \\
  = E_x \left( \boldsymbol{\theta}^* \right) +
  2 \left( \boldsymbol{\theta}^* - \boldsymbol{\theta} \right)^T \cdot
  \left( A^T \cdot \boldsymbol{k}_x - A^T \cdot A \cdot \boldsymbol{\theta}^* \right) +
  \left( \boldsymbol{\theta}^* - \boldsymbol{\theta} \right)^T \cdot A^T \cdot A \cdot
  \left( \boldsymbol{\theta}^* - \boldsymbol{\theta} \right).
\end{gather*}
Якщо $\boldsymbol{\theta}^* = \hat{\boldsymbol{\theta}}$, то
$ E_x \left( \boldsymbol{\theta} \right) =
  E_x \left( \hat{\boldsymbol{\theta}} \right) +
  \left( \hat{\boldsymbol{\theta}} - \boldsymbol{\theta} \right)^T \cdot A^T \cdot A \cdot
  \left( \hat{\boldsymbol{\theta}} - \boldsymbol{\theta} \right) \geq
  E_x \left( \hat{\boldsymbol{\theta}} \right) $,
оскільки матриця $A^T \cdot A$ невід'ємно визначена.
Таким чином, мінімум $E_x \left( \boldsymbol{\theta} \right)$ дорівнює
$E_x \left( \hat{\boldsymbol{\theta}} \right) $,
він досягається при $\boldsymbol{\theta} = \hat{\boldsymbol{\theta}}$.

Для невиродженої матриці $A^T \cdot A$ рівняння \eqref{eq:equation_theta}
однозначно розв'язується, тому
в цьому випадку оцінка найменших квадратів єдина та має вигляд
\eqref{eq:solution}.
Теорему доведено.

\section{Статистичні властивості оцінки найменших квадратів}

\textbf{Теорема.}
Оцінка $\hat{\boldsymbol{\theta}}$ є незміщеною оцінкою параметра $\boldsymbol{\theta}$.

Відомо, що $M\boldsymbol{k}_x = A \cdot \boldsymbol{\theta}$.
Отже,
$M \hat{\boldsymbol{\theta}} =
  \left( A^T \cdot A \right)^{-1} \cdot A^T \cdot M\boldsymbol{k}_x =
  \left( A^T \cdot A \right) \cdot A^T \cdot A \cdot \boldsymbol{\theta}$,
тобто $M\hat{\boldsymbol{\theta}} = \boldsymbol{\theta}$.

\textbf{Теорема.}
В класі оцінок $\boldsymbol{\theta}^*$ величини $\boldsymbol{\theta}$, які
\begin{enumerate}
  \item є незміщеними оцінками,
  \item представляють собою лінійні комбінації вихідних даних $\boldsymbol{k}_x$,
\end{enumerate}
за допомогою критерія найменших
квадратів можна знайти таку оцінку невідомого параметра $\boldsymbol{\theta}$,
що $D\hat{\theta}_j \leq D \theta_j^*$ для будь-якого $j$.
Іншими словами,
$\hat{\boldsymbol{\theta}}$ є оптимальною оцінкою $\boldsymbol{\theta}$ з усіх можливих,
що належать даному класу \cite{hudson:least-squares}.

Щоб довести цю теорему, насамперед треба знайти вираз для $D \theta_j$.
З постановки задачі відомо, що $D\boldsymbol{k}_x = \sigma^2 \cdot I$.
З \eqref{eq:solution} отримуємо
\begin{equation*}
  D\hat{\boldsymbol{\theta}} =
  \left( A^T \cdot A \right)^{-1} \cdot A^T \cdot D\boldsymbol{k}_x \cdot A \cdot
  \left( A^T \cdot A \right)^{-1} =
  \sigma^2 \cdot \left( A^T \cdot A \right)^{-1}.
\end{equation*}
Нехай $\boldsymbol{\theta}^*$~---~якась інша оцінка $\boldsymbol{\theta}$,
яка є лінійною комбінацією вихідних даних,
тобто $\boldsymbol{\theta}^* = U \cdot \boldsymbol{k}_x$.
Вимагаємо, щоб вона була незміщеною:
\begin{equation*}
  M\boldsymbol{\theta}^* =
  U \cdot M\boldsymbol{k}_x =
  U \cdot A \cdot \boldsymbol{\theta} =
  \boldsymbol{\theta}.
\end{equation*}
Остання рівність виконується для всіх $\boldsymbol{\theta}$, тому
\begin{equation*}
  U \cdot A = I.
\end{equation*}
Знайдемо коваріаційну матрицю
$D\boldsymbol{\theta}^* =
  U \cdot D\boldsymbol{k}_x \cdot U^T =
  \sigma^2 \cdot U \cdot U^T$.
Порівняємо $U \cdot U^T$ з $ \left( A^T \cdot A \right)^{-1}$.
Нехай $K = \left( A^T \cdot A \right)^{-1}$.
Тоді можемо записати
\begin{equation*}
  U \cdot U^T =
  K + \left( U - K \cdot A^T \right) \cdot \left( U - K \cdot A^T \right)^T.
\end{equation*}
Ця рівність справедлива завдяки тому, що $U \cdot A = I$.
Таким чином,
\begin{equation*}
  D\boldsymbol{\theta}^* =
  D\hat{\boldsymbol{\theta}} +
  \sigma^2 \left( U - K \cdot A^T \right) \cdot
  \left( U - K \cdot A^T \right)^T.
\end{equation*}
Для $\hat{\boldsymbol{\theta}}$ маємо $U = K \cdot A^T$,
тому другий доданок обертається в нуль.
Для будь-якої іншої оцінки цей доданок невід'ємний, тому
кожен діагональний елемент матриці $D\boldsymbol{\theta}^*$
не менше відповідного діагонального елемента матриці $D\hat{\boldsymbol{\theta}}$.
Теорему доведено.

\section*{Висновки до розділу 2}
\addcontentsline{toc}{section}{Висновки до розділу 2}

Пред'явлено розв'язок задачі ідентифікації випадкових відображень точкових
множин методом найменших квадратів.
За певних умов цей розв'язок єдиний.
За фіксованої розмітки отримані оцінки оптимальні.
Даний метод не враховує нелінійні обмеження, що накладені на шукані параметри:
матриця повороту $R$ має бути ортогональною,
а її визначник повинен дорівнювати одиниці.
Тобто не гарантується, що матриця $R$ буде матрицею повороту.
